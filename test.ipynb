{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a671e289",
   "metadata": {},
   "source": [
    "# 0 Estrutura e Configuração do Projeto\n",
    "- A Fase 0 tem como objetivo garantir que todo o ambiente experimental está corretamente organizado e configurado antes de qualquer processamento de imagens médicas ou extração de features. Esta etapa é crítica para assegurar reprodutibilidade, rastreabilidade e robustez em projetos de Deep Learning aplicados a imagens médicas, tal como recomendado nas boas práticas de engenharia de dados e ciência reprodutível.\n",
    "\n",
    "- Nesta fase são criadas todas as pastas necessárias, definido o ficheiro de configuração global (`config.yaml`), validados os parâmetros críticos e fixadas as seeds para garantir consistência nos resultados. Além disso, são preparados os caminhos e variáveis que serão utilizados ao longo de todo o pipeline, incluindo nas fases posteriores de extração de volumes, criação de crops 3D e obtenção de embeddings com CNNs tridimensionais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57bf9a",
   "metadata": {},
   "source": [
    "###  0.1. Estrutura de Pastas\n",
    "- Cria a hierarquia de diretórios necessária para o pipeline completo.\n",
    "\n",
    "- Estas pastas garantem a separação entre dados brutos, processados e resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae756ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Estrutura de pastas criada/verificada:\n",
      "  └── dicom_raw: data/dicom_raw\n",
      "  └── cache_npz: data/cache_npz\n",
      "  └── nodules_crops: data/nodules_crops\n",
      "  └── embeddings: data/embeddings\n",
      "  └── splits: splits\n",
      "  └── cfg: cfg\n",
      "  └── src: src\n",
      "\n",
      " Configuração guardada em: /home/megu/Code/labiacd/lung-cancer-classification-project-1/cfg/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- Diretório raiz do projeto ---\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# --- Estrutura de pastas ---\n",
    "PATHS = {\n",
    "    \"dicom_raw\": PROJECT_ROOT / \"data\" / \"dicom_raw\",\n",
    "    \"cache_npz\": PROJECT_ROOT / \"data\" / \"cache_npz\",\n",
    "    \"nodules_crops\": PROJECT_ROOT / \"data\" / \"nodules_crops\",\n",
    "    \"embeddings\": PROJECT_ROOT / \"data\" / \"embeddings\",\n",
    "    \"splits\": PROJECT_ROOT / \"splits\",\n",
    "    \"cfg\": PROJECT_ROOT / \"cfg\",\n",
    "    \"src\": PROJECT_ROOT / \"src\",\n",
    "}\n",
    "\n",
    "# Criar pastas\n",
    "for p in PATHS.values():\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\" Estrutura de pastas criada/verificada:\")\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"  └── {k}: {v.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "# --- Criar dicionário de configuração ---\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"hu_window\": {\"min\": -1000, \"max\": 400},\n",
    "    \"target_spacing\": [1.0, 1.0, 1.0],\n",
    "    \"crop_size\": [48, 48, 48],\n",
    "    \"nodule_diameter_mm\": {\"min\": 3.0, \"max\": 30.0},\n",
    "    \"paths\": {k: str(v.relative_to(PROJECT_ROOT)) for k, v in PATHS.items()},\n",
    "    \"training\": {\"batch_size\": 8, \"mixed_precision\": True, \"num_workers\": 4},\n",
    "}\n",
    "\n",
    "# Atualizar o caminho real dos DICOMs (manifest LIDC)\n",
    "CFG[\"paths\"][\"dicom_raw\"] = \"/home/megu/Code/labiacd/LIDC-IDRI/\"\n",
    "\n",
    "# Caminho do ficheiro de configuração\n",
    "CFG_PATH = PATHS[\"cfg\"] / \"config.yaml\"\n",
    "\n",
    "# Função auxiliar para guardar YAML\n",
    "def yaml_dump(data):\n",
    "    return yaml.dump(data, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# Guardar config\n",
    "CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n Configuração guardada em: {CFG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96800da",
   "metadata": {},
   "source": [
    "- verificar que foi tudo feito corretamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e3e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura OK \n",
      "seed: 42\n",
      "hu_window:\n",
      "  min: -1000\n",
      "  max: 400\n",
      "target_spacing:\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "crop_size:\n",
      "- 48\n",
      "- 48\n",
      "- 48\n",
      "nodule_diameter_mm:\n",
      "  min: 3.0\n",
      "  max: 30.0\n",
      "paths:\n",
      "  dicom_raw: /home/megu/Code/labiacd/LIDC-IDRI/\n",
      "  cache_npz: data/cache_npz\n",
      "  nodules_crops: data/nodules_crops\n",
      "  embeddings: data/embeddings\n",
      "  splits: splits\n",
      "  cfg: cfg\n",
      "  src: src\n",
      "training:\n",
      "  batch_size: 8\n",
      "  mixed_precision: true\n",
      "  num_workers: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "cfg_test = yaml.safe_load(open(CFG_PATH))\n",
    "print(\"Leitura OK \")\n",
    "print(yaml.dump(cfg_test, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e0b52",
   "metadata": {},
   "source": [
    "| Pasta                 | Conteúdo                                                        | Motivo / Função                                                                        |\n",
    "| --------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| `data/dicom_raw/`     | DICOMs originais (CTs brutos), organizados por paciente/estudo. | Mantém os dados de origem intactos e reprodutíveis. É a “fonte da verdade” do dataset. |\n",
    "| `data/cache_npz/`     | Volumes reamostrados e normalizados em HU.                      | Evita reler e converter DICOMs a cada execução, garantindo consistência e eficiência.  |\n",
    "| `data/nodules_crops/` | Cubos 3D centrados nos nódulos.                                 | Fornece os *inputs diretos* para as CNNs 3D ou *feature extractors*.                   |\n",
    "| `data/embeddings/`    | Vetores de *features* extraídos após a CNN.                     | Servem de entrada para classificadores (ex.: SVM, XGBoost) em fases posteriores.       |\n",
    "| `splits/`             | Listas `train/val/test` por paciente.                           | Evita *data leakage* entre doentes, assegurando separação correta.                     |\n",
    "| `cfg/`                | Ficheiros de configuração (`config.yaml`).                      | Centraliza os hiperparâmetros e permite reprodutibilidade.                             |\n",
    "| `src/`                | Código modular (funções auxiliares).                            | Mantém o notebook limpo e facilita a reutilização de funções.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe89a2",
   "metadata": {},
   "source": [
    "- A estrutura modularizada segue as recomendações de Pianykh (2012) em DICOM: A Practical Introduction and Survival Guide e as boas práticas propostas por Wilson et al. (2017) em Good Enough Practices in Scientific Computing, que defendem a separação entre dados brutos, processados e resultados para assegurar rastreabilidade total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f820bd",
   "metadata": {},
   "source": [
    "### 0.2 Ficheiro de Configuração Global (`config.yaml`)\n",
    "- Cria ou lê o ficheiro (`cfg/config.yaml`), garantindo consistência entre runs.\n",
    "\n",
    "- O ficheiro (`config.yaml`) funciona como o núcleo paramétrico do projeto, garantindo que todos os valores (como tamanho do crop, janela HU e batch size) são centralizados e facilmente reprodutíveis. Este tipo de configuração externa é considerado uma boa prática em Machine Learning Engineering (Beazley, Python Cookbook, O’Reilly), pois separa a lógica experimental dos parâmetros experimentais, promovendo transparência e versionamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c0739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config carregada/escrita em: cfg/config.yaml\n",
      "{'seed': 42, 'hu_window': {'min': -1000, 'max': 400}, 'target_spacing': [1.0, 1.0, 1.0], 'crop_size': [48, 48, 48], 'nodule_diameter_mm': {'min': 3.0, 'max': 30.0}, 'paths': {'dicom_raw': '/home/megu/Code/labiacd/LIDC-IDRI/', 'cache_npz': 'data/cache_npz', 'nodules_crops': 'data/nodules_crops', 'embeddings': 'data/embeddings', 'splits': 'splits', 'cfg': 'cfg', 'src': 'src'}, 'training': {'batch_size': 8, 'mixed_precision': True, 'num_workers': 4}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"hu_window\": {\"min\": -1000, \"max\": 400},\n",
    "    \"target_spacing\": [1.0, 1.0, 1.0],\n",
    "    \"crop_size\": [48, 48, 48],\n",
    "    \"nodule_diameter_mm\": {\"min\": 3.0, \"max\": 30.0},\n",
    "    \"paths\": {k: str(v.relative_to(PROJECT_ROOT)) for k, v in PATHS.items()},\n",
    "    \"training\": {\"batch_size\": 8, \"mixed_precision\": True, \"num_workers\": 4},\n",
    "}\n",
    "\n",
    "CFG_DIR = PATHS[\"cfg\"]\n",
    "CFG_PATH = CFG_DIR / \"config.yaml\"\n",
    "\n",
    "def yaml_load(text: str) -> dict:\n",
    "    try:\n",
    "        import yaml\n",
    "        return yaml.safe_load(text) or {}\n",
    "    except Exception:\n",
    "        d = {}\n",
    "        for line in text.splitlines():\n",
    "            if \":\" in line:\n",
    "                k, v = line.split(\":\", 1)\n",
    "                d[k.strip()] = v.strip()\n",
    "        return d\n",
    "\n",
    "def yaml_dump(obj: dict) -> str:\n",
    "    try:\n",
    "        import yaml\n",
    "        return yaml.safe_dump(obj, sort_keys=False, allow_unicode=True)\n",
    "    except Exception:\n",
    "        import pprint\n",
    "        return pprint.pformat(obj, sort_dicts=False)\n",
    "\n",
    "def deep_update(base: dict, new: dict) -> dict:\n",
    "    for k, v in (new or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if CFG_PATH.exists():\n",
    "    user_cfg = yaml_load(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    CFG = deep_update(DEFAULT_CFG.copy(), user_cfg)\n",
    "else:\n",
    "    CFG = DEFAULT_CFG.copy()\n",
    "    CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(\" Config carregada/escrita em:\", CFG_PATH.relative_to(PROJECT_ROOT))\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153947c",
   "metadata": {},
   "source": [
    "### 0.3 Validação da Configuração\n",
    "- Confere coerência entre parâmetros críticos como HU window, spacing e crop size.\n",
    "\n",
    "- Esta célula assegura a consistência interna da configuração, prevenindo erros futuros no pipeline. Parâmetros como o windowing HU e o resampling spacing são baseados em recomendações de Gonzalez & Woods (2018) no livro Digital Image Processing, que destaca a importância da uniformidade na aquisição e transformação de imagens médicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df95852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: 'crop_size' tem lados pares. Usar valores ímpares melhora a centragem do nódulo.\n",
      "Validações concluídas.\n"
     ]
    }
   ],
   "source": [
    "def _as_list(x, n=None, typ=float):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        out = list(x)\n",
    "    else:\n",
    "        out = [x]\n",
    "    if n is not None and len(out) != n:\n",
    "        raise ValueError(f\"Esperava lista de tamanho {n}, obtido {out}\")\n",
    "    return [typ(v) for v in out]\n",
    "\n",
    "hu_min = float(CFG[\"hu_window\"][\"min\"])\n",
    "hu_max = float(CFG[\"hu_window\"][\"max\"])\n",
    "if not (hu_min < hu_max):\n",
    "    raise ValueError(f\"hu_window inválido: min={hu_min} >= max={hu_max}\")\n",
    "\n",
    "spacing = _as_list(CFG[\"target_spacing\"], 3, float)\n",
    "if not all(s > 0 for s in spacing):\n",
    "    raise ValueError(f\"target_spacing tem valores não positivos: {spacing}\")\n",
    "\n",
    "crop = _as_list(CFG[\"crop_size\"], 3, int)\n",
    "if any(c % 2 == 0 for c in crop):\n",
    "    print(\"Aviso: 'crop_size' tem lados pares. Usar valores ímpares melhora a centragem do nódulo.\")\n",
    "\n",
    "print(\"Validações concluídas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9a3b9",
   "metadata": {},
   "source": [
    "- Escolha do tamanho do crop. Neste trabalho uso 48×48×48 porque é mais leve e compatível com a maioria das CNNs 3D pré-treinadas, acelerando a extração de features e reduzindo uso de memória — especialmente relevante sem GPU. A opção 49³ tem elegância geométrica (voxel central perfeito), mas o ganho prático nas embeddings é marginal face ao custo extra. Como o foco aqui é feature extraction (não segmentação de margens finas), 48³ oferece o melhor equilíbrio entre precisão e eficiência; se mais tarde eu precisar de centragem exata, posso recortar novamente em 49³ sem alterar o restante pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6d85d",
   "metadata": {},
   "source": [
    "### 0.4 Fixar Seeds e Determinismo\n",
    "- Garante reprodutibilidade de resultados em TensorFlow ou PyTorch.\n",
    "\n",
    "- A fixação de seeds e ativação de determinismo garantem que o modelo se comporta da mesma forma em execuções repetidas. Esta abordagem é fundamental em investigação científica, pois assegura que os resultados são replicáveis — conceito amplamente discutido por Goodfellow, Bengio e Courville em Deep Learning (MIT Press, 2016), que enfatizam a importância do controlo de aleatoriedade em experiências de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32f1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: none\n",
      "• Mixed precision: desativada\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "seed = int(CFG.get(\"seed\", 42))\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ.setdefault(\"PYTHONHASHSEED\", str(seed))\n",
    "os.environ.setdefault(\"TF_DETERMINISTIC_OPS\", \"1\")\n",
    "\n",
    "BACKEND = \"none\"\n",
    "mp_enabled = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    BACKEND = \"tensorflow\"\n",
    "    tf.random.set_seed(seed)\n",
    "    for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    if CFG[\"training\"][\"mixed_precision\"]:\n",
    "        from tensorflow.keras import mixed_precision as mp\n",
    "        mp.set_global_policy(\"mixed_float16\")\n",
    "        mp_enabled = True\n",
    "except Exception:\n",
    "    try:\n",
    "        import torch\n",
    "        BACKEND = \"pytorch\"\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        BACKEND = \"none\"\n",
    "\n",
    "print(f\"Backend: {BACKEND}\")\n",
    "print(f\"• Mixed precision: {'ativada' if mp_enabled else 'desativada'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d591a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch não está disponível: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "# Diagnóstico rápido de mixed precision no PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA disponível:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    from pprint import pprint\n",
    "    print(\"\\ntraining cfg:\")\n",
    "    pprint(CFG.get(\"training\", {}))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch não está disponível:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd62e3",
   "metadata": {},
   "source": [
    "- Sobre o uso de mixed precision: A mixed precision training foi desativada porque o ambiente atual não dispõe de GPU (CUDA = False). Este modo utiliza operações em float16 para acelerar o treino em GPUs modernas, mas em CPU não traz benefícios e pode até reduzir o desempenho. Por esse motivo, a configuração mantém mixed_precision: false, podendo ser reativada futuramente quando o projeto for executado com suporte CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05ea4b",
   "metadata": {},
   "source": [
    "### 0.5 Guardar Configuração Normalizada e Resumo\n",
    "- Esta última célula persiste a configuração normalizada e imprime um resumo completo, garantindo transparência sobre o estado atual do ambiente antes de avançar para a Fase 1 (Leitura de DICOMs e Conversão para HU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047d6428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config final guardada em: cfg/config.yaml\n",
      "\n",
      "Resumo do Projeto:\n",
      "seed: 42\n",
      "hu_window: {'min': -1000, 'max': 400}\n",
      "target_spacing: [1.0, 1.0, 1.0]\n",
      "crop_size: [48, 48, 48]\n",
      "nodule_diameter_mm: {'min': 3.0, 'max': 30.0}\n",
      "paths: {'dicom_raw': '/home/megu/Code/labiacd/LIDC-IDRI/', 'cache_npz': 'data/cache_npz', 'nodules_crops': 'data/nodules_crops', 'embeddings': 'data/embeddings', 'splits': 'splits', 'cfg': 'cfg', 'src': 'src'}\n",
      "training: {'batch_size': 8, 'mixed_precision': True, 'num_workers': 4}\n"
     ]
    }
   ],
   "source": [
    "CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Config final guardada em:\", CFG_PATH.relative_to(PROJECT_ROOT))\n",
    "print(\"\\nResumo do Projeto:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d9dce",
   "metadata": {},
   "source": [
    "## 1. DICOM → HU → Reamostragem Isotrópica (1 mm)\n",
    "\n",
    "- A Fase 1 transforma séries DICOM em volumes 3D em HU, windowizados ([-1000, 400]), normalizados para [0,1] e reamostrados para 1×1×1 mm, guardando tudo em (`data/cache_npz/{patient}.npz`).\n",
    "Isto estabiliza a geometria e acelera as fases seguintes (crops 3D e embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330fd020",
   "metadata": {},
   "source": [
    "### 1.1 Leitura da série DICOM (ordenada) + metadados\n",
    "- o manifeste esta da forma : (`…/LIDC-IDRI/LIDC-IDRI-0001/01-01-2000-…/3000566.000000-NA-03192/*.dcm`). Nos defenimos uma serie como uma paste que contem um conjunto de (`.dcm`) do mesmo exame (tipicamente dezenas/centenas).Sendo que cada paciente pode ter varias series \n",
    "\n",
    "- esta função que criamos usa (`rglob(*.dcm)`) para percorrer todos os níveis e deteta como séries as pastas que têm pelo menos N ficheiros DICOM (por defeito, min_dicoms=10).\n",
    "\n",
    "- Assim, não importa quão fundo esteja a pasta da série , será encontrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a5a5a",
   "metadata": {},
   "source": [
    "- Ordenamos as slices por (`ImagePositionPatient[2]`) (eixo Z). Se faltar, usamos (`InstanceNumber`) como fallback.\n",
    "\n",
    "- Empilhamos em [D,H,W] (D = nº de slices), preservando a fidelidade espacial.\n",
    "\n",
    "- Calculamos o spacing [dz, dy, dx] em mm a partir de:\n",
    "\n",
    "    - Z: diferença entre as posições (IPP[2]) de duas slices consecutivas (mais fiável que  SliceThickness); fallback para SliceThickness se não existir IPP.\n",
    "    \n",
    "    - Y,X: PixelSpacing = [dy, dx].\n",
    "    \n",
    "Isto assegura que o volume respeita a geometria do doente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b21ca",
   "metadata": {},
   "source": [
    "- função utilitaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7e2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dicom_series_dirs(root: Path, min_dicoms: int = 10):\n",
    "    \"\"\"\n",
    "    Percorre recursivamente 'root' e devolve as pastas que contêm\n",
    "    pelo menos 'min_dicoms' ficheiros DICOM (*.dcm).\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    series_dirs = []\n",
    "    for dirpath in root.rglob(\"*\"):\n",
    "        if dirpath.is_dir():\n",
    "            dcm_files = list(dirpath.glob(\"*.dcm\"))\n",
    "            if len(dcm_files) >= min_dicoms:\n",
    "                series_dirs.append(dirpath)\n",
    "    return series_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1e084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "def load_dicom_series(series_dir: Path):\n",
    "    \"\"\"\n",
    "    Lê uma série DICOM (pasta com .dcm) e devolve:\n",
    "      - vol_raw: volume [D,H,W] em int16 (raw)\n",
    "      - spacing: np.array([dz, dy, dx], mm)\n",
    "      - origin: ImagePositionPatient do primeiro slice (se existir)\n",
    "      - slope, intercept: para converter para HU\n",
    "      - meta: dicionário mínimo (UIDs, PatientID)\n",
    "    \"\"\"\n",
    "    series_dir = Path(series_dir)\n",
    "    files = sorted([p for p in series_dir.glob(\"*.dcm\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Sem DICOMs em {series_dir}\")\n",
    "\n",
    "    # Ler todos os cabeçalhos (se tiver transferência comprimida, instala 'pylibjpeg' ou 'gdcm')\n",
    "    slices = [pydicom.dcmread(str(f), stop_before_pixels=False) for f in files]\n",
    "\n",
    "    # Ordenar por posição no paciente (fallback: InstanceNumber)\n",
    "    def sort_key(ds):\n",
    "        if hasattr(ds, \"ImagePositionPatient\"):\n",
    "            return float(ds.ImagePositionPatient[2])  # eixo Z (crescentes)\n",
    "        return int(getattr(ds, \"InstanceNumber\", 0))\n",
    "    slices.sort(key=sort_key)\n",
    "\n",
    "    # Volume [D,H,W] (raw)\n",
    "    vol_raw = np.stack([ds.pixel_array for ds in slices]).astype(np.int16)\n",
    "\n",
    "    # Spacing (z, y, x)\n",
    "    try:\n",
    "        ipp0, ipp1 = slices[0].ImagePositionPatient, slices[1].ImagePositionPatient\n",
    "        dz = abs(ipp1[2] - ipp0[2])\n",
    "    except Exception:\n",
    "        dz = float(getattr(slices[0], \"SliceThickness\", 1.0))\n",
    "    dy, dx = getattr(slices[0], \"PixelSpacing\", [1.0, 1.0])\n",
    "    spacing = np.array([dz, float(dy), float(dx)], dtype=np.float32)\n",
    "\n",
    "    origin = np.array(getattr(slices[0], \"ImagePositionPatient\", [0,0,0]), dtype=np.float32)\n",
    "    slope = float(getattr(slices[0], \"RescaleSlope\", 1.0))\n",
    "    intercept = float(getattr(slices[0], \"RescaleIntercept\", 0.0))\n",
    "    meta = {\n",
    "        \"SeriesInstanceUID\": getattr(slices[0], \"SeriesInstanceUID\", None),\n",
    "        \"StudyInstanceUID\": getattr(slices[0], \"StudyInstanceUID\", None),\n",
    "        \"PatientID\": getattr(slices[0], \"PatientID\", series_dir.name),\n",
    "    }\n",
    "    return vol_raw, spacing, origin, slope, intercept, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e093049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Série selecionada:\n",
      "/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "(127 ficheiros)\n",
      "\n",
      " Volume shape: (127, 512, 512) | dtype: int16\n",
      " Spacing [dz,dy,dx]: [2.5      0.859375 0.859375] | Origin: [-229.   -235.   -335.75]\n",
      " Slope / Intercept: 1.0 -1024.0\n",
      " PatientID: LIDC-IDRI-0135\n",
      "\n",
      " Validação 1.1 concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# 1) Verifica que a config existe\n",
    "assert \"CFG\" in globals(), \"CFG não definido. Corre a célula 0.x primeiro (configuração).\"\n",
    "\n",
    "# 2) Caminho base dos DICOMs (manifest LIDC)\n",
    "dicom_root = Path(CFG[\"paths\"][\"dicom_raw\"])\n",
    "assert dicom_root.exists(), f\"Caminho inválido: {dicom_root}\"\n",
    "\n",
    "# 3) Encontrar uma pasta com >=10 ficheiros DICOM\n",
    "series_dirs = find_dicom_series_dirs(dicom_root, min_dicoms=10)\n",
    "assert series_dirs, f\"Nenhuma série encontrada em {dicom_root}. Confirma o caminho.\"\n",
    "\n",
    "# usar a primeira série encontrada (ou muda o índice para testar outra)\n",
    "test_series = series_dirs[0]\n",
    "print(f\"📂 Série selecionada:\\n{test_series}\\n({len(list(test_series.glob('*.dcm')))} ficheiros)\")\n",
    "\n",
    "# 4) Ler volume com a função implementada na Fase 1\n",
    "vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(test_series)\n",
    "\n",
    "# 5) Mostrar resultados básicos\n",
    "print(\"\\n Volume shape:\", vol_raw.shape, \"| dtype:\", vol_raw.dtype)\n",
    "print(\" Spacing [dz,dy,dx]:\", spacing, \"| Origin:\", origin)\n",
    "print(\" Slope / Intercept:\", slope, intercept)\n",
    "print(\" PatientID:\", meta.get(\"PatientID\"))\n",
    "\n",
    "# 6) Checks automáticos\n",
    "assert vol_raw.ndim == 3 and vol_raw.dtype == np.int16, \"Volume deve ser 3D e int16\"\n",
    "assert spacing.shape == (3,) and (spacing > 0).all(), \"Spacing inválido\"\n",
    "assert np.isfinite(vol_raw).all(), \"Volume contém NaN ou Inf\"\n",
    "\n",
    "print(\"\\n Validação 1.1 concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cbbca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR: /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "vol_raw shape: (127, 512, 512) dtype: int16\n",
      "spacing [dz,dy,dx]: [2.5      0.859375 0.859375]\n",
      "origin: [-229.   -235.   -335.75]\n",
      "slope/intercept: 1.0 -1024.0\n",
      "PatientID: LIDC-IDRI-0135\n"
     ]
    }
   ],
   "source": [
    "# Escolhe uma pasta-série real do manifest (usa o scanner da 1.4 para listar)\n",
    "from pathlib import Path\n",
    "test_series = next(iter(find_dicom_series_dirs(Path(CFG[\"paths\"][\"dicom_raw\"]), min_dicoms=10)))  # 1ª série encontrada\n",
    "\n",
    "vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(test_series)\n",
    "\n",
    "print(\"DIR:\", test_series)\n",
    "print(\"vol_raw shape:\", vol_raw.shape, \"dtype:\", vol_raw.dtype)\n",
    "print(\"spacing [dz,dy,dx]:\", spacing)\n",
    "print(\"origin:\", origin)\n",
    "print(\"slope/intercept:\", slope, intercept)\n",
    "print(\"PatientID:\", meta.get(\"PatientID\"))\n",
    "\n",
    "# checks\n",
    "assert vol_raw.ndim == 3 and vol_raw.dtype == np.int16\n",
    "assert spacing.shape == (3,) and (spacing > 0).all()\n",
    "assert np.isfinite(vol_raw).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69179006",
   "metadata": {},
   "source": [
    "### 1.2 HU + windowing + normalização [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb85f8",
   "metadata": {},
   "source": [
    "\n",
    "- HU = slope * pixel + intercept (valores do cabeçalho DICOM).\n",
    "\n",
    "- Guardamos como float32. HU dá-nos uma escala física e comparável entre exames e máquinas.\n",
    "\n",
    "- Fazemos clip(HU, -1000, 400) — janela que cobre parênquima pulmonar e estruturas relevantes para nódulos.\n",
    "\n",
    "- Normalizamos essa janela para [0,1].\n",
    "\n",
    "- Resultado: intensidades comparáveis, sem saturações extremas que atrapalham a rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17890278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_hu(volume_int16, slope: float, intercept: float):\n",
    "    hu = volume_int16.astype(np.float32) * slope + intercept\n",
    "    return hu.astype(np.float32)\n",
    "\n",
    "def window_and_normalize(hu_volume: np.ndarray, wl_min: float, wl_max: float, out_range=(0.0, 1.0)):\n",
    "    v = np.clip(hu_volume, wl_min, wl_max)\n",
    "    lo, hi = out_range\n",
    "    v = (v - wl_min) / (wl_max - wl_min + 1e-8)\n",
    "    v = v * (hi - lo) + lo\n",
    "    return v.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640152a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU stats  (min/mean/max): -2048.0 -729.3543090820312 3071.0\n",
      "WIN stats (min/mean/max): 0.0 0.35128092765808105 1.0\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "hu = to_hu(vol_raw, slope, intercept)\n",
    "v  = window_and_normalize(hu, CFG[\"hu_window\"][\"min\"], CFG[\"hu_window\"][\"max\"], out_range=(0.0,1.0))\n",
    "\n",
    "print(\"HU stats  (min/mean/max):\", float(hu.min()), float(hu.mean()), float(hu.max()))\n",
    "print(\"WIN stats (min/mean/max):\", float(v.min()),  float(v.mean()),  float(v.max()))\n",
    "print(\"dtype:\", v.dtype)\n",
    "\n",
    "# checks\n",
    "assert hu.dtype == np.float32 and v.dtype == np.float32\n",
    "assert np.isfinite(hu).all() and np.isfinite(v).all()\n",
    "# depois do window, valores ficam em [0,1] (com tolerância numérica)\n",
    "assert v.min() >= -1e-5 and v.max() <= 1+1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ae58c",
   "metadata": {},
   "source": [
    "### 1.3 Reamostragem isotrópica (1×1×1 mm)\n",
    "- Queremos voxels cúbicos com 1 mm de lado: target_spacing = [1,1,1].\n",
    "\n",
    "- Calculamos os fatores de zoom como spacing_in / target.\n",
    "\n",
    "- Interpolamos o volume com ordem 1 (linear) (imagem).\n",
    "\n",
    "    - Quando trabalhares com máscaras (segmentações), usa ordem 0 (nearest) para não criar rótulos fracionários.\n",
    "    \n",
    "- Resultado: volume_iso [D', H', W'], com spacing_out = [1,1,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deea095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "\n",
    "def resample_isotropic(volume: np.ndarray, spacing_in, target=(1.0,1.0,1.0), order=1):\n",
    "    spacing_in = np.array(spacing_in, dtype=np.float32)\n",
    "    target = np.array(target, dtype=np.float32)\n",
    "    # volume está [D,H,W] ≡ [z,y,x] → fatores na mesma ordem\n",
    "    zoom_factors = spacing_in / target\n",
    "    v_iso = zoom(volume, zoom=zoom_factors, order=order)\n",
    "    return v_iso.astype(np.float32), zoom_factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31dea566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom_factors [z,y,x]: [2.5      0.859375 0.859375]\n",
      "vol_iso shape: (318, 440, 440) dtype: float32\n"
     ]
    }
   ],
   "source": [
    "vol_iso, zoom_f = resample_isotropic(v, spacing, target=CFG[\"target_spacing\"], order=1)\n",
    "\n",
    "print(\"zoom_factors [z,y,x]:\", zoom_f)\n",
    "print(\"vol_iso shape:\", vol_iso.shape, \"dtype:\", vol_iso.dtype)\n",
    "\n",
    "# checks\n",
    "assert vol_iso.ndim == 3 and vol_iso.dtype == np.float32\n",
    "assert np.isfinite(vol_iso).all()\n",
    "assert np.allclose(zoom_f, spacing / np.array(CFG[\"target_spacing\"], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980242",
   "metadata": {},
   "source": [
    "### 1.4 Scanner recursivo (manifest LIDC-IDRI) + processamento e cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e1e28",
   "metadata": {},
   "source": [
    "- varre todos os níveis do manifest, identifica pastas-série (≥N DICOMs), processa e guarda data/cache_npz/{patient_id}.npz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "258832fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " séries encontradas: 1018\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081\n",
      "\n",
      " Processamento concluído (amostra):\n",
      "{'patient_id': 'LIDC-IDRI-0135', 'in_shape': (127, 512, 512), 'out_shape': (318, 440, 440), 'spacing_in': (2.5, 0.859375, 0.859375), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (2.5, 0.859375, 0.859375), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0135.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704'}\n",
      "{'patient_id': 'LIDC-IDRI-0545', 'in_shape': (461, 512, 512), 'out_shape': (288, 400, 400), 'spacing_in': (0.625, 0.78125, 0.78125), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (0.625, 0.78125, 0.78125), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0545.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863'}\n",
      "{'patient_id': 'LIDC-IDRI-0998', 'in_shape': (107, 512, 512), 'out_shape': (321, 380, 380), 'spacing_in': (3.0, 0.7421875, 0.7421875), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (3.0, 0.7421875, 0.7421875), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0998.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def find_dicom_series_dirs(root: Path, min_dicoms=10):\n",
    "    \"\"\"Varre recursivamente; devolve pastas que parecem ser séries (>= min_dicoms .dcm).\"\"\"\n",
    "    root = Path(root)\n",
    "    series_dirs = []\n",
    "    for dirpath in root.rglob(\"*\"):\n",
    "        if not dirpath.is_dir():\n",
    "            continue\n",
    "        if len(list(dirpath.glob(\"*.dcm\"))) >= min_dicoms:\n",
    "            series_dirs.append(dirpath)\n",
    "    return series_dirs\n",
    "\n",
    "def process_patient_series(series_dir: Path, cfg: dict, out_dir: Path):\n",
    "    vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(series_dir)\n",
    "    vol_hu = to_hu(vol_raw, slope, intercept)\n",
    "\n",
    "    wl = cfg[\"hu_window\"]\n",
    "    vol_w = window_and_normalize(vol_hu, wl[\"min\"], wl[\"max\"], out_range=(0.0, 1.0))\n",
    "\n",
    "    vol_iso, zoom_f = resample_isotropic(vol_w, spacing, target=cfg[\"target_spacing\"], order=1)\n",
    "\n",
    "    # sanity\n",
    "    assert np.isfinite(vol_iso).all(), \"NaN/Inf no volume reamostrado.\"\n",
    "    patient_id = (meta.get(\"PatientID\") or series_dir.name)\n",
    "\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"{patient_id}.npz\"\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        volume=vol_iso.astype(np.float32),\n",
    "        spacing=np.array(cfg[\"target_spacing\"], dtype=np.float32),\n",
    "        origin=origin.astype(np.float32),\n",
    "        meta=json.dumps(meta),\n",
    "    )\n",
    "    return {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"in_shape\": tuple(vol_raw.shape),\n",
    "        \"out_shape\": tuple(vol_iso.shape),\n",
    "        \"spacing_in\": tuple(map(float, spacing)),\n",
    "        \"spacing_out\": tuple(cfg[\"target_spacing\"]),\n",
    "        \"zoom_factors\": tuple(map(float, zoom_f)),\n",
    "        \"out_path\": str(out_path),\n",
    "        \"series_dir\": str(series_dir),\n",
    "    }\n",
    "\n",
    "# --- Executar num subconjunto (ajusta 'n_series') ---\n",
    "raw_root = Path(CFG[\"paths\"][\"dicom_raw\"]) if Path(CFG[\"paths\"][\"dicom_raw\"]).is_absolute() else (PROJECT_ROOT / CFG[\"paths\"][\"dicom_raw\"])\n",
    "series_dirs = find_dicom_series_dirs(raw_root, min_dicoms=10)\n",
    "print(f\" séries encontradas: {len(series_dirs)}\")\n",
    "for p in series_dirs[:3]:\n",
    "    print(\"  •\", p)\n",
    "\n",
    "out_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "n_series = min(3, len(series_dirs))   # começa por 3 para testar\n",
    "reports = [process_patient_series(s, CFG, out_dir) for s in series_dirs[:n_series]]\n",
    "print(\"\\n Processamento concluído (amostra):\")\n",
    "for r in reports:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8400d",
   "metadata": {},
   "source": [
    "- primeiro verificamos se o scanner vê tudo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0d26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séries detetadas: 1018\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704   (#dcm: 127 )\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863   (#dcm: 461 )\n",
      "  • /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081   (#dcm: 107 )\n"
     ]
    }
   ],
   "source": [
    "raw_root = Path(CFG[\"paths\"][\"dicom_raw\"])\n",
    "series_dirs = find_dicom_series_dirs(raw_root, min_dicoms=10)\n",
    "\n",
    "print(\"Séries detetadas:\", len(series_dirs))\n",
    "for p in series_dirs[:3]:\n",
    "    print(\"  •\", p, \"  (#dcm:\", len(list(p.glob(\"*.dcm\"))), \")\")\n",
    "\n",
    "assert len(series_dirs) > 0, \"Nenhuma série encontrada — confere o caminho do manifest e a profundidade.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea8ef1",
   "metadata": {},
   "source": [
    "- De seguida verificamos se a cache esta correta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa69eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "rep = process_patient_series(series_dirs[0], CFG, out_dir)\n",
    "\n",
    "print(\"Relatório:\", rep)\n",
    "npz = np.load(rep[\"out_path\"])\n",
    "print(\"NPZ keys:\", list(npz.keys()))\n",
    "print(\"volume:\", npz[\"volume\"].shape, npz[\"volume\"].dtype, \" spacing:\", npz[\"spacing\"])\n",
    "\n",
    "# checks\n",
    "assert Path(rep[\"out_path\"]).exists()\n",
    "assert \"volume\" in npz and \"spacing\" in npz and \"meta\" in npz\n",
    "assert npz[\"volume\"].dtype == np.float32\n",
    "assert np.allclose(npz[\"spacing\"], np.array(CFG[\"target_spacing\"], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec74491",
   "metadata": {},
   "source": [
    "### 1.5 Sanity checks visuais (axial/sagital/coronal):\n",
    "\n",
    "- Sanity visual (3 cortes ortogonais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_slices(volume: np.ndarray, title: str = \"\"):\n",
    "    \"\"\"\n",
    "    Mostra 3 vistas ortogonais (axial, sagital, coronal) de um volume 3D.\n",
    "    Espera volume normalizado (ex.: [0,1]) com shape [D,H,W].\n",
    "    \"\"\"\n",
    "    assert volume.ndim == 3, f\"Esperava [D,H,W], recebi {volume.shape}\"\n",
    "    D, H, W = volume.shape\n",
    "    zc, yc, xc = D // 2, H // 2, W // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(volume[zc], cmap=\"gray\");       axes[0].set_title(\"Axial\")\n",
    "    axes[1].imshow(volume[:, yc, :], cmap=\"gray\"); axes[1].set_title(\"Sagital\")\n",
    "    axes[2].imshow(volume[:, :, xc], cmap=\"gray\"); axes[2].set_title(\"Coronal\")\n",
    "    for ax in axes: ax.axis(\"off\")\n",
    "    if title: fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Se acabaste de correr a 1.4, podes usar 'reports':\n",
    "try:\n",
    "    _ = reports  # apenas para verificar se existe\n",
    "    npz_info = reports[0]\n",
    "    npz_path = Path(npz_info[\"out_path\"])\n",
    "    print(\"Usando volume de:\", npz_path)\n",
    "except NameError:\n",
    "    # 2) Caso contrário, escolhe o primeiro .npz disponível na cache:\n",
    "    cache_dir = Path(PROJECT_ROOT) / CFG[\"paths\"][\"cache_npz\"]\n",
    "    npz_path = sorted(cache_dir.glob(\"*.npz\"))[0]\n",
    "    print(\"Usando volume de:\", npz_path)\n",
    "\n",
    "data = np.load(npz_path)\n",
    "vol = data[\"volume\"]  # esperado: float32, normalizado\n",
    "print(\"shape:\", vol.shape, \"| dtype:\", vol.dtype, \"| min/max:\", float(vol.min()), float(vol.max()))\n",
    "\n",
    "# checks rápidos\n",
    "assert vol.ndim == 3, \"Volume deve ser [D,H,W]\"\n",
    "assert vol.dtype == np.float32, \"Volume deve ser float32\"\n",
    "assert np.isfinite(vol).all(), \"Volume tem NaN/Inf\"\n",
    "assert -1e-4 <= vol.min() <= 0.05 and 0.95 <= vol.max() <= 1.0001, \"Range esperado ~[0,1] após window/normalize\"\n",
    "\n",
    "# mostra cortes\n",
    "show_slices(vol, title=f\"{npz_path.stem}  {vol.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73843131",
   "metadata": {},
   "source": [
    "### 1.6 Criar um index.csv do cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c217c",
   "metadata": {},
   "source": [
    "- criamos  splits/cache_index.csv com inventário de volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def build_cache_index(cache_dir: Path, index_csv: Path):\n",
    "    rows = []\n",
    "    npz_files = list(sorted(Path(cache_dir).glob(\"*.npz\")))\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(f\"Sem .npz em {cache_dir}. Processa pacientes na 1.4 primeiro.\")\n",
    "    for npz_path in npz_files:\n",
    "        npz = np.load(npz_path)\n",
    "        vol = npz[\"volume\"]\n",
    "        rows.append({\n",
    "            \"patient_id\": npz_path.stem,\n",
    "            \"path_npz\": str(npz_path),\n",
    "            \"shape_d\": int(vol.shape[0]),\n",
    "            \"shape_h\": int(vol.shape[1]),\n",
    "            \"shape_w\": int(vol.shape[2]),\n",
    "        })\n",
    "    index_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(index_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    return index_csv\n",
    "\n",
    "# Exemplo:\n",
    "# cache_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "# idx_path = PROJECT_ROOT / \"splits\" / \"cache_index.csv\"\n",
    "# print(\"Index gerado em:\", build_cache_index(cache_dir, idx_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971f115",
   "metadata": {},
   "source": [
    "- verificamos que fizemos esta etapa corretamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_path = PROJECT_ROOT / \"splits\" / \"cache_index.csv\"\n",
    "print(\"Index gerado em:\", build_cache_index(PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"], idx_path))\n",
    "print(Path(idx_path).read_text().splitlines()[:5])  # ver primeiras linhas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pylidc)",
   "language": "python",
   "name": "pylidc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
