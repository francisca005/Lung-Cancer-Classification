{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a671e289",
   "metadata": {},
   "source": [
    "# 0 Estrutura e Configura√ß√£o do Projeto\n",
    "- A Fase 0 tem como objetivo garantir que todo o ambiente experimental est√° corretamente organizado e configurado antes de qualquer processamento de imagens m√©dicas ou extra√ß√£o de features. Esta etapa √© cr√≠tica para assegurar reprodutibilidade, rastreabilidade e robustez em projetos de Deep Learning aplicados a imagens m√©dicas, tal como recomendado nas boas pr√°ticas de engenharia de dados e ci√™ncia reprodut√≠vel.\n",
    "\n",
    "- Nesta fase s√£o criadas todas as pastas necess√°rias, definido o ficheiro de configura√ß√£o global (`config.yaml`), validados os par√¢metros cr√≠ticos e fixadas as seeds para garantir consist√™ncia nos resultados. Al√©m disso, s√£o preparados os caminhos e vari√°veis que ser√£o utilizados ao longo de todo o pipeline, incluindo nas fases posteriores de extra√ß√£o de volumes, cria√ß√£o de crops 3D e obten√ß√£o de embeddings com CNNs tridimensionais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57bf9a",
   "metadata": {},
   "source": [
    "###  0.1. Estrutura de Pastas\n",
    "- Cria a hierarquia de diret√≥rios necess√°ria para o pipeline completo.\n",
    "\n",
    "- Estas pastas garantem a separa√ß√£o entre dados brutos, processados e resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae756ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Estrutura de pastas criada/verificada:\n",
      "  ‚îî‚îÄ‚îÄ dicom_raw: data/dicom_raw\n",
      "  ‚îî‚îÄ‚îÄ cache_npz: data/cache_npz\n",
      "  ‚îî‚îÄ‚îÄ nodules_crops: data/nodules_crops\n",
      "  ‚îî‚îÄ‚îÄ embeddings: data/embeddings\n",
      "  ‚îî‚îÄ‚îÄ splits: splits\n",
      "  ‚îî‚îÄ‚îÄ cfg: cfg\n",
      "  ‚îî‚îÄ‚îÄ src: src\n",
      "\n",
      " Configura√ß√£o guardada em: /home/megu/Code/labiacd/lung-cancer-classification-project-1/cfg/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- Diret√≥rio raiz do projeto ---\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# --- Estrutura de pastas ---\n",
    "PATHS = {\n",
    "    \"dicom_raw\": PROJECT_ROOT / \"data\" / \"dicom_raw\",\n",
    "    \"cache_npz\": PROJECT_ROOT / \"data\" / \"cache_npz\",\n",
    "    \"nodules_crops\": PROJECT_ROOT / \"data\" / \"nodules_crops\",\n",
    "    \"embeddings\": PROJECT_ROOT / \"data\" / \"embeddings\",\n",
    "    \"splits\": PROJECT_ROOT / \"splits\",\n",
    "    \"cfg\": PROJECT_ROOT / \"cfg\",\n",
    "    \"src\": PROJECT_ROOT / \"src\",\n",
    "}\n",
    "\n",
    "# Criar pastas\n",
    "for p in PATHS.values():\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\" Estrutura de pastas criada/verificada:\")\n",
    "for k, v in PATHS.items():\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ {k}: {v.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "# --- Criar dicion√°rio de configura√ß√£o ---\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"hu_window\": {\"min\": -1000, \"max\": 400},\n",
    "    \"target_spacing\": [1.0, 1.0, 1.0],\n",
    "    \"crop_size\": [48, 48, 48],\n",
    "    \"nodule_diameter_mm\": {\"min\": 3.0, \"max\": 30.0},\n",
    "    \"paths\": {k: str(v.relative_to(PROJECT_ROOT)) for k, v in PATHS.items()},\n",
    "    \"training\": {\"batch_size\": 8, \"mixed_precision\": True, \"num_workers\": 4},\n",
    "}\n",
    "\n",
    "# Atualizar o caminho real dos DICOMs (manifest LIDC)\n",
    "CFG[\"paths\"][\"dicom_raw\"] = \"/home/megu/Code/labiacd/LIDC-IDRI/\"\n",
    "\n",
    "# Caminho do ficheiro de configura√ß√£o\n",
    "CFG_PATH = PATHS[\"cfg\"] / \"config.yaml\"\n",
    "\n",
    "# Fun√ß√£o auxiliar para guardar YAML\n",
    "def yaml_dump(data):\n",
    "    return yaml.dump(data, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# Guardar config\n",
    "CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n Configura√ß√£o guardada em: {CFG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96800da",
   "metadata": {},
   "source": [
    "- verificar que foi tudo feito corretamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e3e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura OK \n",
      "seed: 42\n",
      "hu_window:\n",
      "  min: -1000\n",
      "  max: 400\n",
      "target_spacing:\n",
      "- 1.0\n",
      "- 1.0\n",
      "- 1.0\n",
      "crop_size:\n",
      "- 48\n",
      "- 48\n",
      "- 48\n",
      "nodule_diameter_mm:\n",
      "  min: 3.0\n",
      "  max: 30.0\n",
      "paths:\n",
      "  dicom_raw: /home/megu/Code/labiacd/LIDC-IDRI/\n",
      "  cache_npz: data/cache_npz\n",
      "  nodules_crops: data/nodules_crops\n",
      "  embeddings: data/embeddings\n",
      "  splits: splits\n",
      "  cfg: cfg\n",
      "  src: src\n",
      "training:\n",
      "  batch_size: 8\n",
      "  mixed_precision: true\n",
      "  num_workers: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "cfg_test = yaml.safe_load(open(CFG_PATH))\n",
    "print(\"Leitura OK \")\n",
    "print(yaml.dump(cfg_test, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e0b52",
   "metadata": {},
   "source": [
    "| Pasta                 | Conte√∫do                                                        | Motivo / Fun√ß√£o                                                                        |\n",
    "| --------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| `data/dicom_raw/`     | DICOMs originais (CTs brutos), organizados por paciente/estudo. | Mant√©m os dados de origem intactos e reprodut√≠veis. √â a ‚Äúfonte da verdade‚Äù do dataset. |\n",
    "| `data/cache_npz/`     | Volumes reamostrados e normalizados em HU.                      | Evita reler e converter DICOMs a cada execu√ß√£o, garantindo consist√™ncia e efici√™ncia.  |\n",
    "| `data/nodules_crops/` | Cubos 3D centrados nos n√≥dulos.                                 | Fornece os *inputs diretos* para as CNNs 3D ou *feature extractors*.                   |\n",
    "| `data/embeddings/`    | Vetores de *features* extra√≠dos ap√≥s a CNN.                     | Servem de entrada para classificadores (ex.: SVM, XGBoost) em fases posteriores.       |\n",
    "| `splits/`             | Listas `train/val/test` por paciente.                           | Evita *data leakage* entre doentes, assegurando separa√ß√£o correta.                     |\n",
    "| `cfg/`                | Ficheiros de configura√ß√£o (`config.yaml`).                      | Centraliza os hiperpar√¢metros e permite reprodutibilidade.                             |\n",
    "| `src/`                | C√≥digo modular (fun√ß√µes auxiliares).                            | Mant√©m o notebook limpo e facilita a reutiliza√ß√£o de fun√ß√µes.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe89a2",
   "metadata": {},
   "source": [
    "- A estrutura modularizada segue as recomenda√ß√µes de Pianykh (2012) em DICOM: A Practical Introduction and Survival Guide e as boas pr√°ticas propostas por Wilson et al. (2017) em Good Enough Practices in Scientific Computing, que defendem a separa√ß√£o entre dados brutos, processados e resultados para assegurar rastreabilidade total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f820bd",
   "metadata": {},
   "source": [
    "### 0.2 Ficheiro de Configura√ß√£o Global (`config.yaml`)\n",
    "- Cria ou l√™ o ficheiro (`cfg/config.yaml`), garantindo consist√™ncia entre runs.\n",
    "\n",
    "- O ficheiro (`config.yaml`) funciona como o n√∫cleo param√©trico do projeto, garantindo que todos os valores (como tamanho do crop, janela HU e batch size) s√£o centralizados e facilmente reprodut√≠veis. Este tipo de configura√ß√£o externa √© considerado uma boa pr√°tica em Machine Learning Engineering (Beazley, Python Cookbook, O‚ÄôReilly), pois separa a l√≥gica experimental dos par√¢metros experimentais, promovendo transpar√™ncia e versionamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c0739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config carregada/escrita em: cfg/config.yaml\n",
      "{'seed': 42, 'hu_window': {'min': -1000, 'max': 400}, 'target_spacing': [1.0, 1.0, 1.0], 'crop_size': [48, 48, 48], 'nodule_diameter_mm': {'min': 3.0, 'max': 30.0}, 'paths': {'dicom_raw': '/home/megu/Code/labiacd/LIDC-IDRI/', 'cache_npz': 'data/cache_npz', 'nodules_crops': 'data/nodules_crops', 'embeddings': 'data/embeddings', 'splits': 'splits', 'cfg': 'cfg', 'src': 'src'}, 'training': {'batch_size': 8, 'mixed_precision': True, 'num_workers': 4}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"hu_window\": {\"min\": -1000, \"max\": 400},\n",
    "    \"target_spacing\": [1.0, 1.0, 1.0],\n",
    "    \"crop_size\": [48, 48, 48],\n",
    "    \"nodule_diameter_mm\": {\"min\": 3.0, \"max\": 30.0},\n",
    "    \"paths\": {k: str(v.relative_to(PROJECT_ROOT)) for k, v in PATHS.items()},\n",
    "    \"training\": {\"batch_size\": 8, \"mixed_precision\": True, \"num_workers\": 4},\n",
    "}\n",
    "\n",
    "CFG_DIR = PATHS[\"cfg\"]\n",
    "CFG_PATH = CFG_DIR / \"config.yaml\"\n",
    "\n",
    "def yaml_load(text: str) -> dict:\n",
    "    try:\n",
    "        import yaml\n",
    "        return yaml.safe_load(text) or {}\n",
    "    except Exception:\n",
    "        d = {}\n",
    "        for line in text.splitlines():\n",
    "            if \":\" in line:\n",
    "                k, v = line.split(\":\", 1)\n",
    "                d[k.strip()] = v.strip()\n",
    "        return d\n",
    "\n",
    "def yaml_dump(obj: dict) -> str:\n",
    "    try:\n",
    "        import yaml\n",
    "        return yaml.safe_dump(obj, sort_keys=False, allow_unicode=True)\n",
    "    except Exception:\n",
    "        import pprint\n",
    "        return pprint.pformat(obj, sort_dicts=False)\n",
    "\n",
    "def deep_update(base: dict, new: dict) -> dict:\n",
    "    for k, v in (new or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if CFG_PATH.exists():\n",
    "    user_cfg = yaml_load(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    CFG = deep_update(DEFAULT_CFG.copy(), user_cfg)\n",
    "else:\n",
    "    CFG = DEFAULT_CFG.copy()\n",
    "    CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(\" Config carregada/escrita em:\", CFG_PATH.relative_to(PROJECT_ROOT))\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153947c",
   "metadata": {},
   "source": [
    "### 0.3 Valida√ß√£o da Configura√ß√£o\n",
    "- Confere coer√™ncia entre par√¢metros cr√≠ticos como HU window, spacing e crop size.\n",
    "\n",
    "- Esta c√©lula assegura a consist√™ncia interna da configura√ß√£o, prevenindo erros futuros no pipeline. Par√¢metros como o windowing HU e o resampling spacing s√£o baseados em recomenda√ß√µes de Gonzalez & Woods (2018) no livro Digital Image Processing, que destaca a import√¢ncia da uniformidade na aquisi√ß√£o e transforma√ß√£o de imagens m√©dicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df95852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: 'crop_size' tem lados pares. Usar valores √≠mpares melhora a centragem do n√≥dulo.\n",
      "Valida√ß√µes conclu√≠das.\n"
     ]
    }
   ],
   "source": [
    "def _as_list(x, n=None, typ=float):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        out = list(x)\n",
    "    else:\n",
    "        out = [x]\n",
    "    if n is not None and len(out) != n:\n",
    "        raise ValueError(f\"Esperava lista de tamanho {n}, obtido {out}\")\n",
    "    return [typ(v) for v in out]\n",
    "\n",
    "hu_min = float(CFG[\"hu_window\"][\"min\"])\n",
    "hu_max = float(CFG[\"hu_window\"][\"max\"])\n",
    "if not (hu_min < hu_max):\n",
    "    raise ValueError(f\"hu_window inv√°lido: min={hu_min} >= max={hu_max}\")\n",
    "\n",
    "spacing = _as_list(CFG[\"target_spacing\"], 3, float)\n",
    "if not all(s > 0 for s in spacing):\n",
    "    raise ValueError(f\"target_spacing tem valores n√£o positivos: {spacing}\")\n",
    "\n",
    "crop = _as_list(CFG[\"crop_size\"], 3, int)\n",
    "if any(c % 2 == 0 for c in crop):\n",
    "    print(\"Aviso: 'crop_size' tem lados pares. Usar valores √≠mpares melhora a centragem do n√≥dulo.\")\n",
    "\n",
    "print(\"Valida√ß√µes conclu√≠das.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9a3b9",
   "metadata": {},
   "source": [
    "- Escolha do tamanho do crop. Neste trabalho uso 48√ó48√ó48 porque √© mais leve e compat√≠vel com a maioria das CNNs 3D pr√©-treinadas, acelerando a extra√ß√£o de features e reduzindo uso de mem√≥ria ‚Äî especialmente relevante sem GPU. A op√ß√£o 49¬≥ tem eleg√¢ncia geom√©trica (voxel central perfeito), mas o ganho pr√°tico nas embeddings √© marginal face ao custo extra. Como o foco aqui √© feature extraction (n√£o segmenta√ß√£o de margens finas), 48¬≥ oferece o melhor equil√≠brio entre precis√£o e efici√™ncia; se mais tarde eu precisar de centragem exata, posso recortar novamente em 49¬≥ sem alterar o restante pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6d85d",
   "metadata": {},
   "source": [
    "### 0.4 Fixar Seeds e Determinismo\n",
    "- Garante reprodutibilidade de resultados em TensorFlow ou PyTorch.\n",
    "\n",
    "- A fixa√ß√£o de seeds e ativa√ß√£o de determinismo garantem que o modelo se comporta da mesma forma em execu√ß√µes repetidas. Esta abordagem √© fundamental em investiga√ß√£o cient√≠fica, pois assegura que os resultados s√£o replic√°veis ‚Äî conceito amplamente discutido por Goodfellow, Bengio e Courville em Deep Learning (MIT Press, 2016), que enfatizam a import√¢ncia do controlo de aleatoriedade em experi√™ncias de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32f1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: none\n",
      "‚Ä¢ Mixed precision: desativada\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "seed = int(CFG.get(\"seed\", 42))\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ.setdefault(\"PYTHONHASHSEED\", str(seed))\n",
    "os.environ.setdefault(\"TF_DETERMINISTIC_OPS\", \"1\")\n",
    "\n",
    "BACKEND = \"none\"\n",
    "mp_enabled = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    BACKEND = \"tensorflow\"\n",
    "    tf.random.set_seed(seed)\n",
    "    for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    if CFG[\"training\"][\"mixed_precision\"]:\n",
    "        from tensorflow.keras import mixed_precision as mp\n",
    "        mp.set_global_policy(\"mixed_float16\")\n",
    "        mp_enabled = True\n",
    "except Exception:\n",
    "    try:\n",
    "        import torch\n",
    "        BACKEND = \"pytorch\"\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        BACKEND = \"none\"\n",
    "\n",
    "print(f\"Backend: {BACKEND}\")\n",
    "print(f\"‚Ä¢ Mixed precision: {'ativada' if mp_enabled else 'desativada'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d591a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch n√£o est√° dispon√≠vel: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "# Diagn√≥stico r√°pido de mixed precision no PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA dispon√≠vel:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    from pprint import pprint\n",
    "    print(\"\\ntraining cfg:\")\n",
    "    pprint(CFG.get(\"training\", {}))\n",
    "except Exception as e:\n",
    "    print(\"PyTorch n√£o est√° dispon√≠vel:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd62e3",
   "metadata": {},
   "source": [
    "- Sobre o uso de mixed precision: A mixed precision training foi desativada porque o ambiente atual n√£o disp√µe de GPU (CUDA = False). Este modo utiliza opera√ß√µes em float16 para acelerar o treino em GPUs modernas, mas em CPU n√£o traz benef√≠cios e pode at√© reduzir o desempenho. Por esse motivo, a configura√ß√£o mant√©m mixed_precision: false, podendo ser reativada futuramente quando o projeto for executado com suporte CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05ea4b",
   "metadata": {},
   "source": [
    "### 0.5 Guardar Configura√ß√£o Normalizada e Resumo\n",
    "- Esta √∫ltima c√©lula persiste a configura√ß√£o normalizada e imprime um resumo completo, garantindo transpar√™ncia sobre o estado atual do ambiente antes de avan√ßar para a Fase 1 (Leitura de DICOMs e Convers√£o para HU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "047d6428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config final guardada em: cfg/config.yaml\n",
      "\n",
      "Resumo do Projeto:\n",
      "seed: 42\n",
      "hu_window: {'min': -1000, 'max': 400}\n",
      "target_spacing: [1.0, 1.0, 1.0]\n",
      "crop_size: [48, 48, 48]\n",
      "nodule_diameter_mm: {'min': 3.0, 'max': 30.0}\n",
      "paths: {'dicom_raw': '/home/megu/Code/labiacd/LIDC-IDRI/', 'cache_npz': 'data/cache_npz', 'nodules_crops': 'data/nodules_crops', 'embeddings': 'data/embeddings', 'splits': 'splits', 'cfg': 'cfg', 'src': 'src'}\n",
      "training: {'batch_size': 8, 'mixed_precision': True, 'num_workers': 4}\n"
     ]
    }
   ],
   "source": [
    "CFG_PATH.write_text(yaml_dump(CFG), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Config final guardada em:\", CFG_PATH.relative_to(PROJECT_ROOT))\n",
    "print(\"\\nResumo do Projeto:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d9dce",
   "metadata": {},
   "source": [
    "## 1. DICOM ‚Üí HU ‚Üí Reamostragem Isotr√≥pica (1 mm)\n",
    "\n",
    "- A Fase 1 transforma s√©ries DICOM em volumes 3D em HU, windowizados ([-1000, 400]), normalizados para [0,1] e reamostrados para 1√ó1√ó1 mm, guardando tudo em (`data/cache_npz/{patient}.npz`).\n",
    "Isto estabiliza a geometria e acelera as fases seguintes (crops 3D e embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330fd020",
   "metadata": {},
   "source": [
    "### 1.1 Leitura da s√©rie DICOM (ordenada) + metadados\n",
    "- o manifeste esta da forma : (`‚Ä¶/LIDC-IDRI/LIDC-IDRI-0001/01-01-2000-‚Ä¶/3000566.000000-NA-03192/*.dcm`). Nos defenimos uma serie como uma paste que contem um conjunto de (`.dcm`) do mesmo exame (tipicamente dezenas/centenas).Sendo que cada paciente pode ter varias series \n",
    "\n",
    "- esta fun√ß√£o que criamos usa (`rglob(*.dcm)`) para percorrer todos os n√≠veis e deteta como s√©ries as pastas que t√™m pelo menos N ficheiros DICOM (por defeito, min_dicoms=10).\n",
    "\n",
    "- Assim, n√£o importa qu√£o fundo esteja a pasta da s√©rie , ser√° encontrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a5a5a",
   "metadata": {},
   "source": [
    "- Ordenamos as slices por (`ImagePositionPatient[2]`) (eixo Z). Se faltar, usamos (`InstanceNumber`) como fallback.\n",
    "\n",
    "- Empilhamos em [D,H,W] (D = n¬∫ de slices), preservando a fidelidade espacial.\n",
    "\n",
    "- Calculamos o spacing [dz, dy, dx] em mm a partir de:\n",
    "\n",
    "    - Z: diferen√ßa entre as posi√ß√µes (IPP[2]) de duas slices consecutivas (mais fi√°vel que  SliceThickness); fallback para SliceThickness se n√£o existir IPP.\n",
    "    \n",
    "    - Y,X: PixelSpacing = [dy, dx].\n",
    "    \n",
    "Isto assegura que o volume respeita a geometria do doente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b21ca",
   "metadata": {},
   "source": [
    "- fun√ß√£o utilitaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7e2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dicom_series_dirs(root: Path, min_dicoms: int = 10):\n",
    "    \"\"\"\n",
    "    Percorre recursivamente 'root' e devolve as pastas que cont√™m\n",
    "    pelo menos 'min_dicoms' ficheiros DICOM (*.dcm).\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    series_dirs = []\n",
    "    for dirpath in root.rglob(\"*\"):\n",
    "        if dirpath.is_dir():\n",
    "            dcm_files = list(dirpath.glob(\"*.dcm\"))\n",
    "            if len(dcm_files) >= min_dicoms:\n",
    "                series_dirs.append(dirpath)\n",
    "    return series_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1e084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "def load_dicom_series(series_dir: Path):\n",
    "    \"\"\"\n",
    "    L√™ uma s√©rie DICOM (pasta com .dcm) e devolve:\n",
    "      - vol_raw: volume [D,H,W] em int16 (raw)\n",
    "      - spacing: np.array([dz, dy, dx], mm)\n",
    "      - origin: ImagePositionPatient do primeiro slice (se existir)\n",
    "      - slope, intercept: para converter para HU\n",
    "      - meta: dicion√°rio m√≠nimo (UIDs, PatientID)\n",
    "    \"\"\"\n",
    "    series_dir = Path(series_dir)\n",
    "    files = sorted([p for p in series_dir.glob(\"*.dcm\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Sem DICOMs em {series_dir}\")\n",
    "\n",
    "    # Ler todos os cabe√ßalhos (se tiver transfer√™ncia comprimida, instala 'pylibjpeg' ou 'gdcm')\n",
    "    slices = [pydicom.dcmread(str(f), stop_before_pixels=False) for f in files]\n",
    "\n",
    "    # Ordenar por posi√ß√£o no paciente (fallback: InstanceNumber)\n",
    "    def sort_key(ds):\n",
    "        if hasattr(ds, \"ImagePositionPatient\"):\n",
    "            return float(ds.ImagePositionPatient[2])  # eixo Z (crescentes)\n",
    "        return int(getattr(ds, \"InstanceNumber\", 0))\n",
    "    slices.sort(key=sort_key)\n",
    "\n",
    "    # Volume [D,H,W] (raw)\n",
    "    vol_raw = np.stack([ds.pixel_array for ds in slices]).astype(np.int16)\n",
    "\n",
    "    # Spacing (z, y, x)\n",
    "    try:\n",
    "        ipp0, ipp1 = slices[0].ImagePositionPatient, slices[1].ImagePositionPatient\n",
    "        dz = abs(ipp1[2] - ipp0[2])\n",
    "    except Exception:\n",
    "        dz = float(getattr(slices[0], \"SliceThickness\", 1.0))\n",
    "    dy, dx = getattr(slices[0], \"PixelSpacing\", [1.0, 1.0])\n",
    "    spacing = np.array([dz, float(dy), float(dx)], dtype=np.float32)\n",
    "\n",
    "    origin = np.array(getattr(slices[0], \"ImagePositionPatient\", [0,0,0]), dtype=np.float32)\n",
    "    slope = float(getattr(slices[0], \"RescaleSlope\", 1.0))\n",
    "    intercept = float(getattr(slices[0], \"RescaleIntercept\", 0.0))\n",
    "    meta = {\n",
    "        \"SeriesInstanceUID\": getattr(slices[0], \"SeriesInstanceUID\", None),\n",
    "        \"StudyInstanceUID\": getattr(slices[0], \"StudyInstanceUID\", None),\n",
    "        \"PatientID\": getattr(slices[0], \"PatientID\", series_dir.name),\n",
    "    }\n",
    "    return vol_raw, spacing, origin, slope, intercept, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e093049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ S√©rie selecionada:\n",
      "/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "(127 ficheiros)\n",
      "\n",
      " Volume shape: (127, 512, 512) | dtype: int16\n",
      " Spacing [dz,dy,dx]: [2.5      0.859375 0.859375] | Origin: [-229.   -235.   -335.75]\n",
      " Slope / Intercept: 1.0 -1024.0\n",
      " PatientID: LIDC-IDRI-0135\n",
      "\n",
      " Valida√ß√£o 1.1 conclu√≠da com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# 1) Verifica que a config existe\n",
    "assert \"CFG\" in globals(), \"CFG n√£o definido. Corre a c√©lula 0.x primeiro (configura√ß√£o).\"\n",
    "\n",
    "# 2) Caminho base dos DICOMs (manifest LIDC)\n",
    "dicom_root = Path(CFG[\"paths\"][\"dicom_raw\"])\n",
    "assert dicom_root.exists(), f\"Caminho inv√°lido: {dicom_root}\"\n",
    "\n",
    "# 3) Encontrar uma pasta com >=10 ficheiros DICOM\n",
    "series_dirs = find_dicom_series_dirs(dicom_root, min_dicoms=10)\n",
    "assert series_dirs, f\"Nenhuma s√©rie encontrada em {dicom_root}. Confirma o caminho.\"\n",
    "\n",
    "# usar a primeira s√©rie encontrada (ou muda o √≠ndice para testar outra)\n",
    "test_series = series_dirs[0]\n",
    "print(f\"üìÇ S√©rie selecionada:\\n{test_series}\\n({len(list(test_series.glob('*.dcm')))} ficheiros)\")\n",
    "\n",
    "# 4) Ler volume com a fun√ß√£o implementada na Fase 1\n",
    "vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(test_series)\n",
    "\n",
    "# 5) Mostrar resultados b√°sicos\n",
    "print(\"\\n Volume shape:\", vol_raw.shape, \"| dtype:\", vol_raw.dtype)\n",
    "print(\" Spacing [dz,dy,dx]:\", spacing, \"| Origin:\", origin)\n",
    "print(\" Slope / Intercept:\", slope, intercept)\n",
    "print(\" PatientID:\", meta.get(\"PatientID\"))\n",
    "\n",
    "# 6) Checks autom√°ticos\n",
    "assert vol_raw.ndim == 3 and vol_raw.dtype == np.int16, \"Volume deve ser 3D e int16\"\n",
    "assert spacing.shape == (3,) and (spacing > 0).all(), \"Spacing inv√°lido\"\n",
    "assert np.isfinite(vol_raw).all(), \"Volume cont√©m NaN ou Inf\"\n",
    "\n",
    "print(\"\\n Valida√ß√£o 1.1 conclu√≠da com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cbbca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR: /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "vol_raw shape: (127, 512, 512) dtype: int16\n",
      "spacing [dz,dy,dx]: [2.5      0.859375 0.859375]\n",
      "origin: [-229.   -235.   -335.75]\n",
      "slope/intercept: 1.0 -1024.0\n",
      "PatientID: LIDC-IDRI-0135\n"
     ]
    }
   ],
   "source": [
    "# Escolhe uma pasta-s√©rie real do manifest (usa o scanner da 1.4 para listar)\n",
    "from pathlib import Path\n",
    "test_series = next(iter(find_dicom_series_dirs(Path(CFG[\"paths\"][\"dicom_raw\"]), min_dicoms=10)))  # 1¬™ s√©rie encontrada\n",
    "\n",
    "vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(test_series)\n",
    "\n",
    "print(\"DIR:\", test_series)\n",
    "print(\"vol_raw shape:\", vol_raw.shape, \"dtype:\", vol_raw.dtype)\n",
    "print(\"spacing [dz,dy,dx]:\", spacing)\n",
    "print(\"origin:\", origin)\n",
    "print(\"slope/intercept:\", slope, intercept)\n",
    "print(\"PatientID:\", meta.get(\"PatientID\"))\n",
    "\n",
    "# checks\n",
    "assert vol_raw.ndim == 3 and vol_raw.dtype == np.int16\n",
    "assert spacing.shape == (3,) and (spacing > 0).all()\n",
    "assert np.isfinite(vol_raw).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69179006",
   "metadata": {},
   "source": [
    "### 1.2 HU + windowing + normaliza√ß√£o [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb85f8",
   "metadata": {},
   "source": [
    "\n",
    "- HU = slope * pixel + intercept (valores do cabe√ßalho DICOM).\n",
    "\n",
    "- Guardamos como float32. HU d√°-nos uma escala f√≠sica e compar√°vel entre exames e m√°quinas.\n",
    "\n",
    "- Fazemos clip(HU, -1000, 400) ‚Äî janela que cobre par√™nquima pulmonar e estruturas relevantes para n√≥dulos.\n",
    "\n",
    "- Normalizamos essa janela para [0,1].\n",
    "\n",
    "- Resultado: intensidades compar√°veis, sem satura√ß√µes extremas que atrapalham a rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17890278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_hu(volume_int16, slope: float, intercept: float):\n",
    "    hu = volume_int16.astype(np.float32) * slope + intercept\n",
    "    return hu.astype(np.float32)\n",
    "\n",
    "def window_and_normalize(hu_volume: np.ndarray, wl_min: float, wl_max: float, out_range=(0.0, 1.0)):\n",
    "    v = np.clip(hu_volume, wl_min, wl_max)\n",
    "    lo, hi = out_range\n",
    "    v = (v - wl_min) / (wl_max - wl_min + 1e-8)\n",
    "    v = v * (hi - lo) + lo\n",
    "    return v.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640152a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU stats  (min/mean/max): -2048.0 -729.3543090820312 3071.0\n",
      "WIN stats (min/mean/max): 0.0 0.35128092765808105 1.0\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "hu = to_hu(vol_raw, slope, intercept)\n",
    "v  = window_and_normalize(hu, CFG[\"hu_window\"][\"min\"], CFG[\"hu_window\"][\"max\"], out_range=(0.0,1.0))\n",
    "\n",
    "print(\"HU stats  (min/mean/max):\", float(hu.min()), float(hu.mean()), float(hu.max()))\n",
    "print(\"WIN stats (min/mean/max):\", float(v.min()),  float(v.mean()),  float(v.max()))\n",
    "print(\"dtype:\", v.dtype)\n",
    "\n",
    "# checks\n",
    "assert hu.dtype == np.float32 and v.dtype == np.float32\n",
    "assert np.isfinite(hu).all() and np.isfinite(v).all()\n",
    "# depois do window, valores ficam em [0,1] (com toler√¢ncia num√©rica)\n",
    "assert v.min() >= -1e-5 and v.max() <= 1+1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ae58c",
   "metadata": {},
   "source": [
    "### 1.3 Reamostragem isotr√≥pica (1√ó1√ó1 mm)\n",
    "- Queremos voxels c√∫bicos com 1 mm de lado: target_spacing = [1,1,1].\n",
    "\n",
    "- Calculamos os fatores de zoom como spacing_in / target.\n",
    "\n",
    "- Interpolamos o volume com ordem 1 (linear) (imagem).\n",
    "\n",
    "    - Quando trabalhares com m√°scaras (segmenta√ß√µes), usa ordem 0 (nearest) para n√£o criar r√≥tulos fracion√°rios.\n",
    "    \n",
    "- Resultado: volume_iso [D', H', W'], com spacing_out = [1,1,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deea095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "\n",
    "def resample_isotropic(volume: np.ndarray, spacing_in, target=(1.0,1.0,1.0), order=1):\n",
    "    spacing_in = np.array(spacing_in, dtype=np.float32)\n",
    "    target = np.array(target, dtype=np.float32)\n",
    "    # volume est√° [D,H,W] ‚â° [z,y,x] ‚Üí fatores na mesma ordem\n",
    "    zoom_factors = spacing_in / target\n",
    "    v_iso = zoom(volume, zoom=zoom_factors, order=order)\n",
    "    return v_iso.astype(np.float32), zoom_factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31dea566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom_factors [z,y,x]: [2.5      0.859375 0.859375]\n",
      "vol_iso shape: (318, 440, 440) dtype: float32\n"
     ]
    }
   ],
   "source": [
    "vol_iso, zoom_f = resample_isotropic(v, spacing, target=CFG[\"target_spacing\"], order=1)\n",
    "\n",
    "print(\"zoom_factors [z,y,x]:\", zoom_f)\n",
    "print(\"vol_iso shape:\", vol_iso.shape, \"dtype:\", vol_iso.dtype)\n",
    "\n",
    "# checks\n",
    "assert vol_iso.ndim == 3 and vol_iso.dtype == np.float32\n",
    "assert np.isfinite(vol_iso).all()\n",
    "assert np.allclose(zoom_f, spacing / np.array(CFG[\"target_spacing\"], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980242",
   "metadata": {},
   "source": [
    "### 1.4 Scanner recursivo (manifest LIDC-IDRI) + processamento e cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e1e28",
   "metadata": {},
   "source": [
    "- varre todos os n√≠veis do manifest, identifica pastas-s√©rie (‚â•N DICOMs), processa e guarda data/cache_npz/{patient_id}.npz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "258832fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " s√©ries encontradas: 1018\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081\n",
      "\n",
      " Processamento conclu√≠do (amostra):\n",
      "{'patient_id': 'LIDC-IDRI-0135', 'in_shape': (127, 512, 512), 'out_shape': (318, 440, 440), 'spacing_in': (2.5, 0.859375, 0.859375), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (2.5, 0.859375, 0.859375), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0135.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704'}\n",
      "{'patient_id': 'LIDC-IDRI-0545', 'in_shape': (461, 512, 512), 'out_shape': (288, 400, 400), 'spacing_in': (0.625, 0.78125, 0.78125), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (0.625, 0.78125, 0.78125), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0545.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863'}\n",
      "{'patient_id': 'LIDC-IDRI-0998', 'in_shape': (107, 512, 512), 'out_shape': (321, 380, 380), 'spacing_in': (3.0, 0.7421875, 0.7421875), 'spacing_out': (1.0, 1.0, 1.0), 'zoom_factors': (3.0, 0.7421875, 0.7421875), 'out_path': '/home/megu/Code/labiacd/lung-cancer-classification-project-1/data/cache_npz/LIDC-IDRI-0998.npz', 'series_dir': '/home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def find_dicom_series_dirs(root: Path, min_dicoms=10):\n",
    "    \"\"\"Varre recursivamente; devolve pastas que parecem ser s√©ries (>= min_dicoms .dcm).\"\"\"\n",
    "    root = Path(root)\n",
    "    series_dirs = []\n",
    "    for dirpath in root.rglob(\"*\"):\n",
    "        if not dirpath.is_dir():\n",
    "            continue\n",
    "        if len(list(dirpath.glob(\"*.dcm\"))) >= min_dicoms:\n",
    "            series_dirs.append(dirpath)\n",
    "    return series_dirs\n",
    "\n",
    "def process_patient_series(series_dir: Path, cfg: dict, out_dir: Path):\n",
    "    vol_raw, spacing, origin, slope, intercept, meta = load_dicom_series(series_dir)\n",
    "    vol_hu = to_hu(vol_raw, slope, intercept)\n",
    "\n",
    "    wl = cfg[\"hu_window\"]\n",
    "    vol_w = window_and_normalize(vol_hu, wl[\"min\"], wl[\"max\"], out_range=(0.0, 1.0))\n",
    "\n",
    "    vol_iso, zoom_f = resample_isotropic(vol_w, spacing, target=cfg[\"target_spacing\"], order=1)\n",
    "\n",
    "    # sanity\n",
    "    assert np.isfinite(vol_iso).all(), \"NaN/Inf no volume reamostrado.\"\n",
    "    patient_id = (meta.get(\"PatientID\") or series_dir.name)\n",
    "\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"{patient_id}.npz\"\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        volume=vol_iso.astype(np.float32),\n",
    "        spacing=np.array(cfg[\"target_spacing\"], dtype=np.float32),\n",
    "        origin=origin.astype(np.float32),\n",
    "        meta=json.dumps(meta),\n",
    "    )\n",
    "    return {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"in_shape\": tuple(vol_raw.shape),\n",
    "        \"out_shape\": tuple(vol_iso.shape),\n",
    "        \"spacing_in\": tuple(map(float, spacing)),\n",
    "        \"spacing_out\": tuple(cfg[\"target_spacing\"]),\n",
    "        \"zoom_factors\": tuple(map(float, zoom_f)),\n",
    "        \"out_path\": str(out_path),\n",
    "        \"series_dir\": str(series_dir),\n",
    "    }\n",
    "\n",
    "# --- Executar num subconjunto (ajusta 'n_series') ---\n",
    "raw_root = Path(CFG[\"paths\"][\"dicom_raw\"]) if Path(CFG[\"paths\"][\"dicom_raw\"]).is_absolute() else (PROJECT_ROOT / CFG[\"paths\"][\"dicom_raw\"])\n",
    "series_dirs = find_dicom_series_dirs(raw_root, min_dicoms=10)\n",
    "print(f\" s√©ries encontradas: {len(series_dirs)}\")\n",
    "for p in series_dirs[:3]:\n",
    "    print(\"  ‚Ä¢\", p)\n",
    "\n",
    "out_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "n_series = min(3, len(series_dirs))   # come√ßa por 3 para testar\n",
    "reports = [process_patient_series(s, CFG, out_dir) for s in series_dirs[:n_series]]\n",
    "print(\"\\n Processamento conclu√≠do (amostra):\")\n",
    "for r in reports:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8400d",
   "metadata": {},
   "source": [
    "- primeiro verificamos se o scanner v√™ tudo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0d26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S√©ries detetadas: 1018\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0135/01-01-2000-NA-CT THORAX WCONTRAST-77417/3.000000-Recon 2 C-A-P-38704   (#dcm: 127 )\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0545/01-01-2000-NA-CHEST-53172/31181.000000-Recon 2 ACRIN LARGE-46863   (#dcm: 461 )\n",
      "  ‚Ä¢ /home/megu/Code/labiacd/LIDC-IDRI/LIDC-IDRI-0998/01-01-2000-NA-NA-51584/5397.000000-ThoraxRoutine  3.0  B31s-66081   (#dcm: 107 )\n"
     ]
    }
   ],
   "source": [
    "raw_root = Path(CFG[\"paths\"][\"dicom_raw\"])\n",
    "series_dirs = find_dicom_series_dirs(raw_root, min_dicoms=10)\n",
    "\n",
    "print(\"S√©ries detetadas:\", len(series_dirs))\n",
    "for p in series_dirs[:3]:\n",
    "    print(\"  ‚Ä¢\", p, \"  (#dcm:\", len(list(p.glob(\"*.dcm\"))), \")\")\n",
    "\n",
    "assert len(series_dirs) > 0, \"Nenhuma s√©rie encontrada ‚Äî confere o caminho do manifest e a profundidade.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea8ef1",
   "metadata": {},
   "source": [
    "- De seguida verificamos se a cache esta correta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa69eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "rep = process_patient_series(series_dirs[0], CFG, out_dir)\n",
    "\n",
    "print(\"Relat√≥rio:\", rep)\n",
    "npz = np.load(rep[\"out_path\"])\n",
    "print(\"NPZ keys:\", list(npz.keys()))\n",
    "print(\"volume:\", npz[\"volume\"].shape, npz[\"volume\"].dtype, \" spacing:\", npz[\"spacing\"])\n",
    "\n",
    "# checks\n",
    "assert Path(rep[\"out_path\"]).exists()\n",
    "assert \"volume\" in npz and \"spacing\" in npz and \"meta\" in npz\n",
    "assert npz[\"volume\"].dtype == np.float32\n",
    "assert np.allclose(npz[\"spacing\"], np.array(CFG[\"target_spacing\"], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec74491",
   "metadata": {},
   "source": [
    "### 1.5 Sanity checks visuais (axial/sagital/coronal):\n",
    "\n",
    "- Sanity visual (3 cortes ortogonais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_slices(volume: np.ndarray, title: str = \"\"):\n",
    "    \"\"\"\n",
    "    Mostra 3 vistas ortogonais (axial, sagital, coronal) de um volume 3D.\n",
    "    Espera volume normalizado (ex.: [0,1]) com shape [D,H,W].\n",
    "    \"\"\"\n",
    "    assert volume.ndim == 3, f\"Esperava [D,H,W], recebi {volume.shape}\"\n",
    "    D, H, W = volume.shape\n",
    "    zc, yc, xc = D // 2, H // 2, W // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(volume[zc], cmap=\"gray\");       axes[0].set_title(\"Axial\")\n",
    "    axes[1].imshow(volume[:, yc, :], cmap=\"gray\"); axes[1].set_title(\"Sagital\")\n",
    "    axes[2].imshow(volume[:, :, xc], cmap=\"gray\"); axes[2].set_title(\"Coronal\")\n",
    "    for ax in axes: ax.axis(\"off\")\n",
    "    if title: fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Se acabaste de correr a 1.4, podes usar 'reports':\n",
    "try:\n",
    "    _ = reports  # apenas para verificar se existe\n",
    "    npz_info = reports[0]\n",
    "    npz_path = Path(npz_info[\"out_path\"])\n",
    "    print(\"Usando volume de:\", npz_path)\n",
    "except NameError:\n",
    "    # 2) Caso contr√°rio, escolhe o primeiro .npz dispon√≠vel na cache:\n",
    "    cache_dir = Path(PROJECT_ROOT) / CFG[\"paths\"][\"cache_npz\"]\n",
    "    npz_path = sorted(cache_dir.glob(\"*.npz\"))[0]\n",
    "    print(\"Usando volume de:\", npz_path)\n",
    "\n",
    "data = np.load(npz_path)\n",
    "vol = data[\"volume\"]  # esperado: float32, normalizado\n",
    "print(\"shape:\", vol.shape, \"| dtype:\", vol.dtype, \"| min/max:\", float(vol.min()), float(vol.max()))\n",
    "\n",
    "# checks r√°pidos\n",
    "assert vol.ndim == 3, \"Volume deve ser [D,H,W]\"\n",
    "assert vol.dtype == np.float32, \"Volume deve ser float32\"\n",
    "assert np.isfinite(vol).all(), \"Volume tem NaN/Inf\"\n",
    "assert -1e-4 <= vol.min() <= 0.05 and 0.95 <= vol.max() <= 1.0001, \"Range esperado ~[0,1] ap√≥s window/normalize\"\n",
    "\n",
    "# mostra cortes\n",
    "show_slices(vol, title=f\"{npz_path.stem}  {vol.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73843131",
   "metadata": {},
   "source": [
    "### 1.6 Criar um index.csv do cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c217c",
   "metadata": {},
   "source": [
    "- criamos  splits/cache_index.csv com invent√°rio de volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def build_cache_index(cache_dir: Path, index_csv: Path):\n",
    "    rows = []\n",
    "    npz_files = list(sorted(Path(cache_dir).glob(\"*.npz\")))\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(f\"Sem .npz em {cache_dir}. Processa pacientes na 1.4 primeiro.\")\n",
    "    for npz_path in npz_files:\n",
    "        npz = np.load(npz_path)\n",
    "        vol = npz[\"volume\"]\n",
    "        rows.append({\n",
    "            \"patient_id\": npz_path.stem,\n",
    "            \"path_npz\": str(npz_path),\n",
    "            \"shape_d\": int(vol.shape[0]),\n",
    "            \"shape_h\": int(vol.shape[1]),\n",
    "            \"shape_w\": int(vol.shape[2]),\n",
    "        })\n",
    "    index_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(index_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    return index_csv\n",
    "\n",
    "# Exemplo:\n",
    "# cache_dir = PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"]\n",
    "# idx_path = PROJECT_ROOT / \"splits\" / \"cache_index.csv\"\n",
    "# print(\"Index gerado em:\", build_cache_index(cache_dir, idx_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971f115",
   "metadata": {},
   "source": [
    "- verificamos que fizemos esta etapa corretamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_path = PROJECT_ROOT / \"splits\" / \"cache_index.csv\"\n",
    "print(\"Index gerado em:\", build_cache_index(PROJECT_ROOT / CFG[\"paths\"][\"cache_npz\"], idx_path))\n",
    "print(Path(idx_path).read_text().splitlines()[:5])  # ver primeiras linhas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pylidc)",
   "language": "python",
   "name": "pylidc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
